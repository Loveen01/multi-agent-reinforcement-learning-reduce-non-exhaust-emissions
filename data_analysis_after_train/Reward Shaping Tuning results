Reward Shaping Tuning results

First, each RL agent in the 4-intersection network was trained with a reward function focused solely on minimizing emissions \eqref{eq:emissions_only_reward}. Intuitively, this resulted in the signal intersection controller eventually stabilizing the red light for a large portion of the simulation period. With minimizing emissions as its sole concern, this yielded a first-order solution to the optimization problem where traffic signals remained red continuously.

Therefore, a congestion penalty term was added \eqref{eq:congestion_penalty_equation}, representing the differences in the sum of waiting times \ref{sec1.2} of all vehicles at the intersectionâ€”a term commonly used in many reinforcement learning problems to optimize for congestion and network flow \cite{}. This term \eqref{eq:congestion_penalty_term_uncapped} was added linearly to the absolute accelerations, forming equation \eqref{eq:composite_reward_with_uncapped_penalty}. With the scales of the two terms being different, both terms were normalized via mean normalization, and a coefficient, alpha, was assigned to the congestion penalty term.

After conducting multiple training experiments using the reward function \eqref{eq:congestion_penalty_term_uncapped}, it was found that the congestion penalty term, although normalised, had added noise to the reward function, that caused it to learn ineffectively. The values of the reward function ranged in both the positive and negative domains, with positive rewards given .... It is believed that this positive reward had interfered with the (negative) emissions component, slightly reducing the overall values. Although the positive rewards credited the agent when congestion was better than usual, this crediting interfered with the rewards obtained from absolute accelerations, causing the reward function to fluctuate rapidly. This added noise resulted in ineffective learning. (Figure - show that the reward was much smoother after capping the reward?)

To improve and simplify the reward function, the congestion penalty term C_i  was capped at 0 when it fell within the positive range (indicating that the current vehicle sum waiting time is less than the last measured vehicle sum of waiting times). By discarding the positive values of this term, it ensured that an immediate negative penalty was applied only when the current waiting time exceeded the previous measure. This approach penalized the agent solely for increased waiting times, without rewarding reductions. The values remained scaled by a factor of 100. This modification resulted in more effective learning, as shown by the learning curves for all alpha coefficients in Figure 4.

% **when assigning positive values when the /eqref{congestion_penalty_term_uncapped} was positive, indicating that the total waiting time recorded at the current simulation step is smaller than the waiting time assigned in the previous waiting step. Figure (3:figure of reward function over time for the normalised, then show the same data for capped OR show data in Appendix?). 

With congestion coefficients in the range 0 - 0.75, being very small, still yet focused solely on reducing emissions, diminishing the aim of the congestion term in the reward function. Whilst the recorded systems tyre emissions is extremely low (3 - 10 emissions/s), the resulting sum of waiting times of the vehicles in the network is ridiculously high, approximately ten times that of its fixed control baseline. This is due to most of the cars coming to a halt at the junctions, causing a reduction in accelerations over a large chunk of the simulation period. At an alpha = 0.75, what was found was that one of the agent had optimised the reward function effectively, but for the other 3 agents, it was found . With a low penalty assigned, it is clear that the training quickly stabilised at having only one agent work well, but the other 3 not. 

Likewise, when the reward function is configured to optimising solely for delta waiting time, the system average waiting time rests as 5s, a value much lower than the fixed control baseline.

At a coefficent above 0.75, the penalisation term begins to take effect as we improvements in the systems mean waiting time, where it scores 20percent higher than the respective fixed time controlled signalised intersections. At above this coefficient the mean waiting time metric surpasses that achieved by the reward function of optimising solely for reduced waiting time (eq 2). As we increase the scale of the coefficients to whole numbers, alpha = [2, 3, 5, 10], there is an overall reduction in the average waiting times, however, the systems total emissions increases from 14 emissions/s to 17 emissions/s (1dp), reaching that produced by the extreme reward function of solely optimising for delta waiting time. Therefore, the alpha deemed suitable for further experiments is the alphas in the range 1-2, at this point this alpha seeks a balance in optimising for average waiting time (being < fixed time control baseline), as well achieving an emission value of less that obtained by fixed time control and that obtained by reward function 3 (fn3 = optimising for delta waiting time).










In terms of the performance of individual agents, this can be said the same for 2 agents in the environment, where the other 2 perform same emissions as the fixed control baseline. For the waiting time, it is less than the original fixed time control baseline,  
0.8 is best so far - it scores better than the fixed control baseline on all metrics. Only for the agent 6, did it score similar to the emissions produced at its fixed control baseline.  

Reducing the observations followed a more greedy approach - > it was more concerned with reducing absolute accelerations of its surroundings. This may be because of the fact that due to a reduction in observations space of all other signalised intersections, our penalty is slightly reduced. 




% diff_accum_wait_time
The RL based agents performance for the 4-intersection were compared to the fixed control strategy. 














Discussion and reflection

Reward Shaping and Tuning 

The art of reward shaping is an art of itself, and sometimes does not have a straight forward answer ... 