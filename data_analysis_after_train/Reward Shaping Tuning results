Reward Shaping Tuning results

First each RL agent in the 4-intersection network was trained with a reward function concerned with only optimising for minimising emissions \eqref{eq:emissions_only_reward}. Intuitively this resulted in the signal intersection controller to eventually stabilise on the red light for a large chunk of the simulation period. With minimising emissions being its sole concern, this had yielded the first order solution to this optimisation problem where the traffic signals halt at red continuously.

Therefore, an congestion penalty term was added, \eqref{eq congestion_penalty_equation}, representing the mean difference in waiting time \ref{sec1.2} of all vehicles at the intersection, a term used alone in many reinforcement learning problems to optimise for congestion and network flow \cite{}. This term \eqref{eq:congestion_penalty_term_uncapped} was added linearly to the absolute accelerations, forming equation \eqref{eq composite_reward_with_uncapped_penalty}. With the scales of the two terms being different, both terms had been normalised via mean normalisation, and a coefficient, alpha, was given to the congestion penalty term. 

After conducting multiple training experiments using the reward function \eqref{eq:congestion_penalty_term_uncapped}, it was found that the congestion penalty term, although normalised, had added noise to the reward function, that caused it to learn ineffectively. This is because the values had ranged in the positive and negative domain, it was given a positive reward ...It was realised that it caused innefective learning, and added noise to the reward function. Figure (3:figure of reward function over time for the normalised, then show the same data for capped). To resolve this, the difference in waiting time was capped the congestion penalty term eq(congestion_penalty), such that the positive range (indicating mean_waiting_time < mean_waiting_time.last_measure) is discarded from the term ref{congestion_penalty_capped}.  Capping this term ensured that every time the waiting time exceeded the last measure, an immediate direct penalty was given to the agent ensuring that it is only penalised when it performed worse, and not rewarded otherwise. The models learning curves are shown for all alpha coefficient in figure 4. 

% equation fn 2 - reward function capped
The reward \( r_i \) for intersection \( i \) is defined as:
\begin{equation}
r_i = \sum_{v \in V_i} \left| a_v \right| + \alpha \sum_{v \in V_i} \Delta w_v
\label{eq:composite_reward_fn}
\end{equation}
where:
\[
\left| a_v \right| = \text{absolute acceleration of vehicle } v \text{ at intersection } i
\]
\[
\Delta w_v = \text{wait\_time\_before} - \text{wait\_time\_now} \text{ for vehicle } v \text{ at intersection } i
\]
\[
V_i = \text{set of all vehicles at the lanes of intersection } i
\]
\[
\alpha = \text{weighting factor}
\]


With congestion coefficients in the range 0 - 0.75, being very small, still yet focused solely on reducing emissions, diminishing the aim of the congestion term in the reward function. Whilst the recorded systems tyre emissions is extremely low (3 - 10 emissions/s), the resulting mean waiting times of the vehicles in the network is ridiculously high, approximately ten times that of its fixed control baseline. This is due to all the cars coming to a halt at the junctions, causing a reduction in accelerations over a large chunk of the simulation period. 

Likewise, when the reward function is configured to optimising solely for delta waiting time, the system average waiting time rests as 5s, a value much lower than the fixed control baseline.

At a coefficent above 0.75, the penalisation term begins to take effect as we improvements in the systems mean waiting time, where it scores 20percent higher than the respective fixed time controlled signalised intersections. At above this coefficient the mean waiting time metric surpasses that achieved by the reward function of optimising solely for reduced waiting time (eq 2). As we increase the scale of the coefficients to whole numbers, alpha = [2, 3, 5, 10], there is an overall reduction in the average waiting times, however, the systems total emissions increases from 14 emissions/s to 17 emissions/s (1dp), reaching that produced by the extreme reward function of solely optimising for delta waiting time. Therefore, the alpha deemed suitable for further experiments is the alphas in the range 1-2, at this point this alpha seeks a balance in optimising for average waiting time (being < fixed time control baseline), as well achieving an emission value of less that obtained by fixed time control and that obtained by reward function 3 (fn3 = optimising for delta waiting time).










In terms of the performance of individual agents, this can be said the same for 2 agents in the environment, where the other 2 perform same emissions as the fixed control baseline. For the waiting time, it is less than the original fixed time control baseline,  
0.8 is best so far - it scores better than the fixed control baseline on all metrics. Only for the agent 6, did it score similar to the emissions produced at its fixed control baseline.  

Reducing the observations followed a more greedy approach - > it was more concerned with reducing absolute accelerations of its surroundings. This may be because of the fact that due to a reduction in observations space of all other signalised intersections, our penalty is slightly reduced. 




% diff_accum_wait_time
The RL based agents performance for the 4-intersection were compared to the fixed control strategy. 