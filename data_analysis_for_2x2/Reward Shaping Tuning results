Reward Shaping Results

Balancing Emission Reduction and Traffic Flow

Initially, each RL agent in the 4-intersection network was trained with a reward function focused solely on minimizing emissions, \eqref{eq:emissions_only_reward}. This strategy naturally inclined the signal intersection controller to maintain the red light for the majority of the simulation period. Consequently, the primary solution to the optimization problem involved keeping the traffic signals continuously red, a direct outcome of the emission-reduction focus.

To address congestion, a congestion penalty term was added \eqref{congestion_penalty_term_uncapped}, representing the differences in the sum of waiting times \ref{sec1.2} of all vehicles at the intersection— a term commonly used in reward functions in many RL network flow optimisations problems \cite{}. This term \eqref{eq:congestion_penalty_term_uncapped} was added linearly to the absolute accelerations, \eqref{eq:composite_reward_with_uncapped_penalty}. Given the differing scales of the two terms, the averages and ranges of both terms were collected during multiple simulations of the environment under fixed-time control. This data was used to inform the mean normalization process of the reward function. Additionally, a coefficient, {\(\alpha\)} was assigned to the congestion penalty term to balance its influence.

After conducting multiple training experiments, it was found that the agents were not learning as effectively as expected. With several high fluctuations in the value function loss graph, it took approximately 150 training iterations before the policies converged, see Figure {Total Loss Capped vs Uncapped}. Large fluctuations are also visible in the graphs of explained variance, indicating that the value function network struggled to predict the reward given a particular state accurately, affecting the Advantage estimations subsequently the policy updates.

From inspection of the reward function, we can see that this is due to the sharp fluctuations in the trajectory from the congestion penalty term. The interference of the positive domain (signifying rewards credited when the agent congestion was better than usual) interfered with the (negative) emissions reward component, adding continuous noise to the reward function. This high variability in the reward function, made it more challenging to approximating the true value of a given state, as shown in {Explained Variance}. 

To resolve this, the congestion penalty term C_i was capped at 0 when its value fell within the positive range, discarding the reward assigned to the agent when the current vehicle sum waiting time is less than that which was last measured. By discarding the positive domain of this term, it ensured that only a negative penalty is applied immediately when the current waiting time exceeded the previous measure. This approach penalized the agent solely for increase in waiting times of the vehicles surrounding the intersection, without rewarding any captured reductions. This modification to the reward function reduced the variability and the moving average convergence of the reward function by approximately factor of 2, as shown in agent's Reward Trajectory Graphs /ref{}, and table /ref{table of moving average and variance}. As a result, the policy's networks value function approximation improved, achieving higher and more stable explained variance, as observed in the explained variance graphs, Figure {5}. 
% GRAPH - Total Loss Capped vs Uncapped
% GRAPH - Total Explained variance 
% GRAPH - Total Agent Reward Trajectory
% TABLE - convergence_summary_table_of_interest
% -----------------------------------------------
% APPENDIX - Explained variance for each agent
% APPENDIX - Total Loss for each agent ?
% APPENDIX - VF Loss for each Agent ? TBC 

Alpha Tuning 

% table showing results for system_wide_metrics alpha 0, delta, 1, 2, 3, 5, 10
% table showing data for agent wide metrics alpha 0, 1, 2, 3, 5, 10

The penalty term coefficient, alpha, was individually trained across a wide range of values under identical training conditions to find an optimal balance between emissions reduction and congestion reduction, each over 150 iterations (150,000 environment steps). Additionally, the two extremes—optimizing solely for emissions, \eqref{eq:emissions_only_reward}, and solely for congestion, \eqref{eq:congestion_penalty_term_uncapped} —were also trained under the same conditions. Figure 3 shows the recorded average waiting times and non-exhaust emissions for each alpha value and the two extremes after evaluating the policies over multiple seeds. A log transformation has been applied, in order to make the plot more readable. 

With congestion coefficients in the range 0 - 0.75, linking to small penalty values, remained focused solely on reducing emissions. Whilst the recorded systems mean tyre emissions is extremely low (3 - 10 emissions/s), the resulting sum of waiting times of the vehicles in the network is ridiculously high, approximately ten times that of its fixed control baseline. This is due to most of the cars coming to a halt at the junctions, causing a reduction in accelerations over a large chunk of the simulation period, as seen similarly in alpha=0 scenarios. 

At an alpha = 0.75, an inspection of the video shown one of the agents had optimised the reward function effectively, but for the other 3 agents, they had stabilised on the practice of causing all cars to halt at the intersection /ref{GRAPH OF ONLY ONE TRAFFIC SIGNAL SHOWING HALTING CARS OTHER 3 FINE}. With a low penalty assigned, it is clear that the training quickly stabilised at having only one agent work well, but the other 3 not. 

However, increasing the coefficient beyond 0.75, caused the agent to reduce in its waiting time, but also strike a balance between reducing waiting times and reducing emissions. 

At alpha in the ranges between alpha = 1 - 2, we find that the systems average waiting times are better than that recorded in the fixed control baseline, whilst also achieving an emissions value of 14.3 emissions/s, lower than the fixed control baseline. 

Likewise, when the reward function is configured to optimising solely for delta waiting time, /eqref{congestion_penalty_capped}, the system average waiting time rests as 5s, a value much lower than the fixed control baseline.

% GRAPH - LEARNING CURVES OF ALPHA = 0 
% GRAPH - LEARNING CURVES OF DELTA_WAIT_TIME

In terms of the performance of individual agents, this can be said the same for 2 agents in the environment, where the other 2 perform same emissions as the fixed control baseline. For the waiting time, it is less than the original fixed time control baseline,  
0.8 is best so far - it scores better than the fixed control baseline on all metrics. Only for the agent 6, did it score similar to the emissions produced at its fixed control baseline.  