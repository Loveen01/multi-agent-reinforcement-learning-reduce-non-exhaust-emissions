Reward Shaping Results

Balancing Emission Reduction and Traffic Flow

Initially, each RL agent in the 4-intersection network was trained with a reward function focused solely on minimizing emissions, \eqref{eq:emissions_only_reward}. This strategy naturally inclined the signal intersection controller to maintain the red light for the majority of the simulation period. Consequently, the primary solution to the optimization problem involved keeping the traffic signals continuously red, a direct outcome of the emission-reduction focus.

To address congestion, a congestion penalty term was added \eqref{congestion_penalty_term_uncapped}, representing the differences in the sum of waiting times \ref{sec1.2} of all vehicles at the intersection— a term commonly used in reward functions in many RL network flow optimisations problems \cite{}. This term \eqref{eq:congestion_penalty_term_uncapped} was added linearly to the absolute accelerations, \eqref{eq:composite_reward_with_uncapped_penalty}. Given the differing scales of the two terms, the averages and ranges of both terms were collected during multiple simulations of the environment under fixed-time control. This data was used to inform the mean normalization process of the reward function. Additionally, a coefficient, {\(\alpha\)} was assigned to the congestion penalty term to balance its influence.

After conducting multiple training experiments, it was found that the agents were not learning as effectively as expected. With several high fluctuations in the value function loss graph, it took approximately 150 training iterations before the policies converged, see Figure {Total Loss Capped vs Uncapped}. Large fluctuations are also visible in the graphs of explained variance, indicating that the value function network struggled to predict the reward given a particular state accurately, affecting the Advantage estimations subsequently the policy updates.

From inspection of the reward function, we can see that this is due to the sharp fluctuations in the trajectory from the congestion penalty term. The interference of the positive domain (signifying rewards credited when the agent congestion was better than usual) interfered with the (negative) emissions reward component, adding continuous noise to the reward function. This high variability in the reward function, made it more challenging to approximating the true value of a given state, as shown in {Explained Variance}. 

To resolve this, the congestion penalty term C_i was capped at 0 when its value fell within the positive range, discarding the reward assigned to the agent when the current vehicle sum waiting time is less than that which was last measured. By discarding the positive domain of this term, it ensured that only a negative penalty is applied immediately when the current waiting time exceeded the previous measure. This approach penalized the agent solely for increase in waiting times of the vehicles surrounding the intersection, without rewarding any captured reductions. This modification to the reward function reduced the variability and the moving average convergence of the reward function by approximately factor of 2, as shown in agent's Reward Trajectory Graphs /ref{}, and table /ref{table of moving average and variance}. As a result, the policy's networks value function approximation improved, achieving higher and more stable explained variance, as observed in the explained variance graphs, Figure {5}. 

% GRAPH - Total Loss Capped vs Uncapped
% GRAPH - Total Explained variance 
% GRAPH - Total Agent Reward Trajectory
% TABLE - convergence_summary_table_of_interest
% -----------------------------------------------
% APPENDIX - Explained variance for each agent
% APPENDIX - Total Loss for each agent ?
% APPENDIX - VF Loss for each Agent ? TBC 

Alpha Tuning 

The penalty term coefficient, alpha, was individually trained across a wide range of values under identical training conditions to find an optimal balance between emissions reduction and congestion reduction, each over 150 iterations (150,000 environment steps). Additionally, the two extremes—optimizing solely for emissions, \eqref{eq:emissions_only_reward}, and solely for congestion, \eqref{eq:congestion_penalty_term_uncapped} —were also trained under the same conditions. Figure 3 shows the recorded average waiting times and non-exhaust emissions for each alpha value and the two extremes after evaluating the policies over multiple seeds. A log transformation has been applied to make the plot more readable.

The results, as illustrated in Figure 3, demonstrate a trade-off between average waiting time and emissions across varying alpha values. At lower alpha values, specifically near zero, the system achieves minimal emissions but at the cost of exceedingly high average waiting times, suggesting a predominance of red signals which induces severe congestion. As alpha increases, a noticeable decline in waiting times is observed, reaching an equilibrium point around alpha = 1 to 2, where a more optimal balance between emissions and waiting times is achieved.

Figure 4 (Total Loss for all coefficients) provides additional insights into the training dynamics for different alpha values. The graph illustrates how total loss evolves over the training iterations for each alpha value. The total loss is composed of policy loss, value function loss, KL divergence loss, and entropy loss. From the graph, we observe a Rapid Convergence for $\alpha = 0$ and $\alpha = 1$: The blue and purple lines, representing $\alpha = 0$ and $\alpha = 1$ respectively, show a steep decline in total loss early in the training process. This indicates faster convergence compared to other alpha values. This is expected, as the reward functions, \ref{} are simple to optimise. However with alpha values 1-3, it took approximately 40 more iterations to finally converge, where the lines for $\alpha = 1$ (purple) and $\alpha = 2$ (black) show relatively stable and low total loss after the initial iterations, indicating a balanced influence of the penalty term and effective learning.

Table \ref{system_wide_metrics_alpha} substantiates these observations by providing system-wide metrics for different alpha values. At alpha = 0, the average waiting time peaks at an exorbitant 181,912.78 seconds, accompanied by minimal emissions of 1 - 5 E/s. Within the range of 0 - 0.75, only one agent learns to optimize effectively, while the other four agents stabilize by halting most vehicles, as observed in the video analysis (see Figure 5).Conversely, at alpha = 1, the waiting time plummets to 9 seconds, with emissions rising moderately to 14.30 E/s, a value lower than its fixed time control baseline.Therefore, an alpha coefficient of 1, which signifies a trade-off between the two metrics, will be used for subsequent experiments.

% GRAPH - LEARNING CURVES OF ALPHA = 0 
% GRAPH - LEARNING CURVES OF DELTA_WAIT_TIME
% TABLE - system_wide_metrics alpha 0, delta, 1, 2, 3, 5, 10


Interestingly, at higher alpha values, such as alpha = 10, there is a slight increase in waiting times and absolute acceleration metrics, suggesting that while emissions are still being managed, the focus shifts more towards congestion management, potentially at the expense of optimal emission reduction.